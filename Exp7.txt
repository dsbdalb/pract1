# Import the English module from Spacy
from spacy.lang.en import English

# Load English tokenizer, tagger, parser, NER and word vectors
nlp = English()

# Define a text to process
text = """He determined to drop his litigation with the monastery, and relinquish his claims to the wood-cutting and 
fishery rights at once. He was the more ready to do this because the rights had become much less valuable, and he had 
indeed the vaguest idea where the wood and river in question were."""

# Create a "nlp" object for linguistic annotations
my_doc = nlp(text)

# Create a list of word tokens
token_list = []
for token in my_doc:
    token_list.append(token.text)

# Import stopwords from Spacy
from spacy.lang.en.stop_words import STOP_WORDS

# Create a list of word tokens after removing stopwords
filtered_sentence = []
for word in token_list:
    lexeme = nlp.vocab[word]
    if lexeme.is_stop == False:
        filtered_sentence.append(word) 

# Print the original token list and the filtered sentence
print(token_list)
print("\n\n", filtered_sentence)

# Download necessary NLTK resources
import nltk
nltk.download("punkt")
nltk.download("stopword")
nltk.download("wordnet")
nltk.download("averaged_perceptron_tagger")

# Import NLTK modules
from nltk import word_tokenize, sent_tokenize

# Tokenize words and sentences
corpus = "Sachin was the GOAT of the previous generation, Virat is the GOAT of this generation, Shubman will be the GOAT of the next generation"

print(word_tokenize(corpus))
print(sent_tokenize(corpus))

from nltk import pos_tag

# Perform part-of-speech tagging
tokens = word_tokenize(corpus)
print(pos_tag(tokens))

from nltk.corpus import stopwords

# Remove stopwords from tokens
stop_words = set(stopwords.words("english"))
tokens = word_tokenize(corpus)
cleaned_tokens = []
for token in tokens:
    if token not in stop_words:
        cleaned_tokens.append(token)
print(cleaned_tokens)

from nltk.stem import PorterStemmer

# Perform stemming using PorterStemmer
stemmer = PorterStemmer()
stemmed_tokens = []
for token in cleaned_tokens:
    stemmed = stemmer.stem(token)
    stemmed_tokens.append(stemmed)
print(stemmed_tokens)

from nltk.stem import WordNetLemmatizer

# Perform lemmatization using WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = []
for token in cleaned_tokens:
    lemmatized = lemmatizer.lemmatize(token)
    lemmatized_tokens.append(lemmatized)
print(lemmatized_tokens)

from sklearn.feature_extraction.text import TfidfVectorizer

# Create a corpus for TF-IDF vectorization
corpus = [
    "Sachin was the GOAT of the previous generation",
    "Virat is the GOAT of this generation",
    "Shubman will be the GOAT of the next generation"
]

# Create a TfidfVectorizer object
vectorizer = TfidfVectorizer()

# Fit the vectorizer on the corpus
matrix = vectorizer.fit(corpus)

# Get the vocabulary of the matrix
matrix.vocabulary_

# Transform the corpus into TF-IDF matrix
tfidf_matrix = vectorizer.transform(corpus)
print(tfidf_matrix)

# Get the feature names (terms) in the TF-IDF matrix
print(vectorizer.get_feature_names_out())
