#Saperate1
import nltk
from nltk.tokenize import word_tokenize

#Function to split text to word
nltk.download('punkt')

tokens = word_tokenize("The Quick Brown Fox Jumps Over The Lazy Dog")

nltk.download ('stopwords')

print(tokens)

from nltk.corpus import stopwords

stop_words = set(stopwords.words("english"))

tokens = [w for w in tokens if not w in stop_words]

print(tokens)

from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()
stems = []
for t in tokens:
    stems.append(porter.stem(t))
    print(stems)

# Download necessary NLTK resources
import nltk
nltk.download("punkt")
nltk.download("stopword")
nltk.download("wordnet")
nltk.download("averaged_perceptron_tagger")

# Import NLTK modules
from nltk import word_tokenize, sent_tokenize

# Tokenize words and sentences
corpus = "Sachin was the GOAT of the previous generation, Virat is the GOAT of this generation, Shubman will be the GOAT of the next generation"

print(word_tokenize(corpus))
print(sent_tokenize(corpus))

from nltk import pos_tag

# Perform part-of-speech tagging
tokens = word_tokenize(corpus)
print(pos_tag(tokens))

from nltk.corpus import stopwords

# Remove stopwords from tokens
stop_words = set(stopwords.words("english"))
tokens = word_tokenize(corpus)
cleaned_tokens = []
for token in tokens:
    if token not in stop_words:
        cleaned_tokens.append(token)
print(cleaned_tokens)

from nltk.stem import PorterStemmer

# Perform stemming using PorterStemmer
stemmer = PorterStemmer()
stemmed_tokens = []
for token in cleaned_tokens:
    stemmed = stemmer.stem(token)
    stemmed_tokens.append(stemmed)
print(stemmed_tokens)

from nltk.stem import WordNetLemmatizer

# Perform lemmatization using WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = []
for token in cleaned_tokens:
    lemmatized = lemmatizer.lemmatize(token)
    lemmatized_tokens.append(lemmatized)
print(lemmatized_tokens)

from sklearn.feature_extraction.text import TfidfVectorizer

# Create a corpus for TF-IDF vectorization
corpus = [
    "Sachin was the GOAT of the previous generation",
    "Virat is the GOAT of this generation",
    "Shubman will be the GOAT of the next generation"
]

# Create a TfidfVectorizer object
vectorizer = TfidfVectorizer()

# Fit the vectorizer on the corpus
matrix = vectorizer.fit(corpus)

# Get the vocabulary of the matrix
matrix.vocabulary_

# Transform the corpus into TF-IDF matrix
tfidf_matrix = vectorizer.transform(corpus)
print(tfidf_matrix)

# Get the feature names (terms) in the TF-IDF matrix
print(vectorizer.get_feature_names_out())
