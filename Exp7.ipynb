{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192ef47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'determined', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', 'his', 'claims', 'to', 'the', 'wood', '-', 'cuting', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', 'He', 'was', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'rights', 'had', 'become', 'much', 'less', 'valuable', ',', 'and', 'he', 'had', '\\n', 'indeed', 'the', 'vaguest', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were', '.']\n",
      "\n",
      "\n",
      " ['determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood', '-', 'cuting', '\\n', 'fishery', 'rihgts', '.', 'ready', 'becuase', 'rights', 'valuable', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Create list of word tokens after removing stopwords\n",
    "filtered_sentence =[]\n",
    "\n",
    "for word in token_list:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_sentence.append(word) \n",
    "print(token_list)\n",
    "print(\"\\n\\n\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068848fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopword\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2d5ab",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43286ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "corpus = \"Sachin was the GOAT of the previous generation, Virat is the GOAT of the this generation, Shubman will be the GOAT of the next generation\"\n",
    "\n",
    "print(word_tokenize(corpus))\n",
    "print(sent_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9523d73",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(corpus)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d7f3b",
   "metadata": {},
   "source": [
    "# Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(corpus)\n",
    "cleaned_tokens = []\n",
    "for token in tokens:\n",
    "if (token not in stop_words):\n",
    "cleaned_tokens.append(token)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179eb24d",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9551263",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = []\n",
    "for token in cleaned_tokens:\n",
    "    stemmed = stemmer.stem(token)\n",
    "    stemmed_tokens.append(stemmed)\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710c75a",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b91cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29950e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = []\n",
    "for token in cleaned_tokens:\n",
    "    lemmatized = lemmatizer.lemmatize(token)\n",
    "    lemmatized_tokens.append(lemmatized)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b38f56",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cf62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Sachin was the GOAT of the previous generation\",\n",
    "    \"Virat is the GOAT of the this generation\",\n",
    "    \"Shubman will be the GOAT of the next generation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = vectorizer.fit(corpus)\n",
    "matrix.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.transform(corpus)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef29ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
